{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\ksred\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\ksred\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ksred\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ksred\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ksred\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ksred\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"AirQualityData-TH.csv\")\n",
    "\n",
    "# Function to fix timestamps\n",
    "def fix_timestamp(row):\n",
    "    date_part, time_part = row.split(\", \")\n",
    "    if time_part.startswith(\"24:\"):\n",
    "        new_date = (datetime.strptime(date_part, \"%m/%d/%Y\") + timedelta(days=1)).strftime(\"%m/%d/%Y\")\n",
    "        new_time = time_part.replace(\"24:\", \"00:\")\n",
    "        return f\"{new_date}, {new_time}\"\n",
    "    return row\n",
    "\n",
    "# Apply fix to ReceivedTime column\n",
    "df[\"ReceivedTime\"] = df[\"ReceivedTime\"].apply(fix_timestamp)\n",
    "\n",
    "# Save to new CSV\n",
    "df.to_csv(\"AirQualityData-TH-fixed.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2 in c:\\users\\ksred\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.9.10)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data import...\n",
      "\n",
      "Data insertion complete:\n",
      "- Successfully inserted: 2000 rows\n",
      "- Failed to insert: 0 rows\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Database connection from URL\n",
    "DATABASE_URL = \"postgresql://postgres:root@localhost:5432/postgres?schema=public\"\n",
    "\n",
    "# Path to your CSV file\n",
    "CSV_FILE_PATH = \"AirQualityData-TH-fixed.csv\"\n",
    "\n",
    "def parse_db_url(db_url):\n",
    "    \"\"\"Extract connection parameters from DATABASE_URL\"\"\"\n",
    "    result = urlparse(db_url)\n",
    "    return {\n",
    "        'host': result.hostname,\n",
    "        'database': result.path[1:],  # Remove leading '/'\n",
    "        'user': result.username,\n",
    "        'password': result.password,\n",
    "        'port': result.port,\n",
    "    }\n",
    "\n",
    "def parse_datetime(dt_str):\n",
    "    \"\"\"Handle multiple timestamp formats\"\"\"\n",
    "    formats = [\n",
    "        '%Y-%m-%dT%H:%M:%S.%fZ',  # With microseconds and Z\n",
    "        '%Y-%m-%dT%H:%M:%SZ',     # Without microseconds, with Z\n",
    "        '%Y-%m-%dT%H:%M:%S.%f',   # With microseconds, no Z\n",
    "        '%Y-%m-%dT%H:%M:%S'       # Without microseconds, no Z\n",
    "    ]\n",
    "    \n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            return datetime.strptime(dt_str, fmt)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    raise ValueError(f\"Time data '{dt_str}' doesn't match any known format\")\n",
    "\n",
    "def get_column_case_insensitive(df, possible_names):\n",
    "    \"\"\"Get column name with case-insensitive matching\"\"\"\n",
    "    for name in possible_names:\n",
    "        if name in df.columns:\n",
    "            return name\n",
    "        if name.lower() in [col.lower() for col in df.columns]:\n",
    "            return [col for col in df.columns if col.lower() == name.lower()][0]\n",
    "    raise KeyError(f\"None of {possible_names} found in DataFrame columns\")\n",
    "\n",
    "def insert_data_from_csv():\n",
    "    try:\n",
    "        # Parse connection parameters\n",
    "        db_params = parse_db_url(DATABASE_URL)\n",
    "        \n",
    "        # Read CSV file\n",
    "        df = pd.read_csv(CSV_FILE_PATH)\n",
    "        \n",
    "        # Find correct column names (handling variations)\n",
    "        pm25_col = get_column_case_insensitive(df, ['PM2.5', 'PM2_5', 'pm25'])\n",
    "        pm10_col = get_column_case_insensitive(df, ['PM10', 'pm10'])\n",
    "        reported_col = get_column_case_insensitive(df, ['ReportedTime-UTC', 'ReportedTime_UTC'])\n",
    "        \n",
    "        # Connect to PostgreSQL\n",
    "        conn = psycopg2.connect(**db_params)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Prepare the insert statement\n",
    "        insert_query = sql.SQL(\"\"\"\n",
    "            INSERT INTO public.air_quality(\n",
    "                sitename, humidity, temperature, noise, pm2_5, pm10, \n",
    "                receivedtime, reportedtime_utc)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\")\n",
    "        \n",
    "        # Counters for success/error\n",
    "        success_count = 0\n",
    "        error_count = 0\n",
    "        \n",
    "        # Iterate through DataFrame rows and insert data\n",
    "        for index, row in df.iterrows():\n",
    "            try:\n",
    "                # Convert date strings to datetime objects\n",
    "                received_time = datetime.strptime(row['ReceivedTime'], '%m/%d/%Y, %H:%M:%S')\n",
    "                reported_time_utc = parse_datetime(row[reported_col])\n",
    "                \n",
    "                # Handle possible NaN/empty values\n",
    "                data = {\n",
    "                    'sitename': row['SiteName'],\n",
    "                    'humidity': float(row['Humidity']) if pd.notna(row['Humidity']) else None,\n",
    "                    'temperature': float(row['Temperature']) if pd.notna(row['Temperature']) else None,\n",
    "                    'noise': int(row['Noise']) if pd.notna(row['Noise']) else None,\n",
    "                    'pm2_5': float(row[pm25_col]) if pd.notna(row[pm25_col]) else None,\n",
    "                    'pm10': float(row[pm10_col]) if pd.notna(row[pm10_col]) else None,\n",
    "                    'received_time': received_time,\n",
    "                    'reported_time_utc': reported_time_utc\n",
    "                }\n",
    "                \n",
    "                # Execute the insert\n",
    "                cursor.execute(insert_query, (\n",
    "                    data['sitename'],\n",
    "                    data['humidity'],\n",
    "                    data['temperature'],\n",
    "                    data['noise'],\n",
    "                    data['pm2_5'],\n",
    "                    data['pm10'],\n",
    "                    data['received_time'],\n",
    "                    data['reported_time_utc']\n",
    "                ))\n",
    "                success_count += 1\n",
    "                \n",
    "            except Exception as row_error:\n",
    "                error_count += 1\n",
    "                print(f\"\\nError inserting row {index + 1}: {str(row_error)}\")\n",
    "                print(\"Problematic row data:\")\n",
    "                print(row.to_dict())\n",
    "                print(\"Current data being processed:\")\n",
    "                print(data if 'data' in locals() else \"Data not processed\")\n",
    "                continue\n",
    "        \n",
    "        # Commit the transaction\n",
    "        conn.commit()\n",
    "        print(f\"\\nData insertion complete:\")\n",
    "        print(f\"- Successfully inserted: {success_count} rows\")\n",
    "        print(f\"- Failed to insert: {error_count} rows\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nMajor error: {str(e)}\")\n",
    "        if 'conn' in locals():\n",
    "            conn.rollback()\n",
    "    finally:\n",
    "        if 'cursor' in locals():\n",
    "            cursor.close()\n",
    "        if 'conn' in locals():\n",
    "            conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(CSV_FILE_PATH):\n",
    "        print(f\"Error: CSV file not found at {CSV_FILE_PATH}\")\n",
    "    else:\n",
    "        print(\"Starting data import...\")\n",
    "        insert_data_from_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting water quality data import...\n",
      "Reading and processing CSV file...\n",
      "Found 2000 rows in CSV file\n",
      "Connecting to database...\n",
      "\n",
      "Starting batch processing (21 batches)...\n",
      "\n",
      "Processing batch 1/21 (rows 1-100)\n",
      "Inserted 100/100 rows successfully\n",
      "Overall progress: 5.0%\n",
      "\n",
      "Processing batch 2/21 (rows 101-200)\n",
      "Inserted 100/100 rows successfully\n",
      "Overall progress: 10.0%\n",
      "\n",
      "Processing batch 3/21 (rows 201-300)\n",
      "Inserted 100/100 rows successfully\n",
      "Overall progress: 15.0%\n",
      "\n",
      "Processing batch 4/21 (rows 301-400)\n",
      "Inserted 100/100 rows successfully\n",
      "Overall progress: 20.0%\n",
      "\n",
      "Processing batch 5/21 (rows 401-500)\n",
      "Inserted 100/100 rows successfully\n",
      "Overall progress: 25.0%\n",
      "\n",
      "Processing batch 6/21 (rows 501-600)\n",
      "Inserted 100/100 rows successfully\n",
      "Overall progress: 30.0%\n",
      "\n",
      "Processing batch 7/21 (rows 601-700)\n",
      "Inserted 100/100 rows successfully\n",
      "Overall progress: 35.0%\n",
      "\n",
      "Processing batch 8/21 (rows 701-800)\n",
      "Inserted 100/100 rows successfully\n",
      "Overall progress: 40.0%\n",
      "\n",
      "Processing batch 9/21 (rows 801-900)\n",
      "Inserted 100/100 rows successfully\n",
      "Overall progress: 45.0%\n",
      "\n",
      "Processing batch 10/21 (rows 901-1000)\n",
      "Inserted 100/100 rows successfully\n",
      "Overall progress: 50.0%\n",
      "\n",
      "Processing batch 11/21 (rows 1001-1100)\n",
      "Inserted 100/100 rows successfully\n",
      "Overall progress: 55.0%\n",
      "\n",
      "Processing batch 12/21 (rows 1101-1200)\n",
      "Inserted 100/100 rows successfully\n",
      "Overall progress: 60.0%\n",
      "\n",
      "Processing batch 13/21 (rows 1201-1300)\n",
      "Inserted 100/100 rows successfully\n",
      "Overall progress: 65.0%\n",
      "\n",
      "Processing batch 14/21 (rows 1301-1400)\n",
      "Inserted 100/100 rows successfully\n",
      "Overall progress: 70.0%\n",
      "\n",
      "Processing batch 15/21 (rows 1401-1500)\n",
      "Inserted 100/100 rows successfully\n",
      "Overall progress: 75.0%\n",
      "\n",
      "Processing batch 16/21 (rows 1501-1600)\n",
      "Inserted 100/100 rows successfully\n",
      "Overall progress: 80.0%\n",
      "\n",
      "Processing batch 17/21 (rows 1601-1700)\n",
      "Inserted 100/100 rows successfully\n",
      "Overall progress: 85.0%\n",
      "\n",
      "Processing batch 18/21 (rows 1701-1800)\n",
      "Inserted 100/100 rows successfully\n",
      "Overall progress: 90.0%\n",
      "\n",
      "Processing batch 19/21 (rows 1801-1900)\n",
      "Inserted 100/100 rows successfully\n",
      "Overall progress: 95.0%\n",
      "\n",
      "Processing batch 20/21 (rows 1901-2000)\n",
      "Inserted 100/100 rows successfully\n",
      "Overall progress: 100.0%\n",
      "\n",
      "Import complete in 1.58 seconds\n",
      "Total rows processed: 2000\n",
      "Successfully inserted: 2000\n",
      "Failed rows: 0\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from psycopg2.extras import Json\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "# Configuration\n",
    "DATABASE_URL = \"postgresql://postgres:root@localhost:5432/postgres?schema=public\"\n",
    "CSV_FILE_PATH = \"ysi_data.csv\"\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# Define NULL handling for all float fields according to Prisma schema\n",
    "FLOAT_FIELDS = {\n",
    "    'cond': None,\n",
    "    'depth': None,\n",
    "    'nlf': None,\n",
    "    'odo_sat': None,\n",
    "    'odo_cb': None,\n",
    "    'odo': None,\n",
    "    'pressure': None,\n",
    "    'sal': None,\n",
    "    'spcond': None,\n",
    "    'tds': None,\n",
    "    'turbidity': None,\n",
    "    'tss': None,\n",
    "    'wiper_position': None,\n",
    "    'temp': None,\n",
    "    'vertical_position': None,\n",
    "}\n",
    "\n",
    "def parse_db_url(db_url: str) -> Dict[str, Any]:\n",
    "    \"\"\"Extract connection parameters from DATABASE_URL\"\"\"\n",
    "    result = urlparse(db_url)\n",
    "    return {\n",
    "        'host': result.hostname,\n",
    "        'database': result.path[1:],\n",
    "        'user': result.username,\n",
    "        'password': result.password,\n",
    "        'port': result.port,\n",
    "    }\n",
    "\n",
    "def clean_float_value(value: Any, field_name: str) -> Optional[float]:\n",
    "    \"\"\"Safely convert any value to float for database insertion\"\"\"\n",
    "    if pd.isna(value) or value in ['', 'NaN', 'nan', 'NAN', None]:\n",
    "        return FLOAT_FIELDS.get(field_name)\n",
    "    \n",
    "    try:\n",
    "        if isinstance(value, str):\n",
    "            value = value.strip().replace(',', '.')\n",
    "            if value.lower() in ['inf', '+inf', '-inf', 'infinity', '+infinity', '-infinity']:\n",
    "                return None\n",
    "            value = ''.join(c for c in value if c.isdigit() or c in '.-+eE')\n",
    "        \n",
    "        float_value = float(value)\n",
    "        if pd.isna(float_value) or float_value in [float('inf'), float('-inf')]:\n",
    "            return None\n",
    "        return float_value\n",
    "        \n",
    "    except (ValueError, TypeError) as e:\n",
    "        print(f\"Warning: Could not convert {field_name} value '{value}' to float: {e}\")\n",
    "        return FLOAT_FIELDS.get(field_name)\n",
    "\n",
    "def parse_timestamp(ts_str: str) -> Optional[datetime]:\n",
    "    \"\"\"Parse timestamps with automatic format detection\"\"\"\n",
    "    if pd.isna(ts_str) or ts_str == '' or ts_str is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        if isinstance(ts_str, str):\n",
    "            ts_str = ts_str.replace('-', '/').replace('\\\\', '/')\n",
    "            \n",
    "            if ' ' in ts_str:\n",
    "                date_part, time_part = ts_str.split(' ', 1)\n",
    "            else:\n",
    "                date_part = ts_str\n",
    "                time_part = '00:00:00'\n",
    "            \n",
    "            date_formats = ['%Y/%m/%d', '%m/%d/%Y', '%d/%m/%Y']\n",
    "            parsed_date = None\n",
    "            for fmt in date_formats:\n",
    "                try:\n",
    "                    parsed_date = datetime.strptime(date_part, fmt)\n",
    "                    break\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            \n",
    "            if not parsed_date:\n",
    "                return None\n",
    "                \n",
    "            time_parts = time_part.split(':')\n",
    "            hour = int(time_parts[0])\n",
    "            minute = int(time_parts[1]) if len(time_parts) > 1 else 0\n",
    "            second = 0\n",
    "            microsecond = 0\n",
    "            \n",
    "            if len(time_parts) > 2:\n",
    "                seconds_part = time_parts[2]\n",
    "                if '.' in seconds_part:\n",
    "                    second = int(seconds_part.split('.')[0])\n",
    "                    microsecond = int(float(f\"0.{seconds_part.split('.')[1]}\") * 1e6)\n",
    "                else:\n",
    "                    second = int(seconds_part)\n",
    "            \n",
    "            return datetime(\n",
    "                parsed_date.year, parsed_date.month, parsed_date.day,\n",
    "                hour, minute, second, microsecond\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing timestamp {ts_str}: {e}\")\n",
    "    return None\n",
    "\n",
    "def transform_payload(payload_str: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Transform payload into JSON object, returns None for non-JSON data\"\"\"\n",
    "    if pd.isna(payload_str) or payload_str == '' or payload_str is None:\n",
    "        return None\n",
    "    \n",
    "    payload_str = payload_str.strip().strip('\"').strip(\"'\")\n",
    "    \n",
    "    try:\n",
    "        # Only return proper JSON objects\n",
    "        payload = json.loads(payload_str)\n",
    "        if isinstance(payload, dict):  # Only accept dictionary-style JSON\n",
    "            return payload\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "\n",
    "def process_csv_file(file_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Read and process CSV file with robust error handling\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            raw_data = f.read(1024)\n",
    "            detected_encoding = 'utf-8'\n",
    "        \n",
    "        for encoding in ['utf-8', 'latin-1', 'windows-1252']:\n",
    "            try:\n",
    "                raw_data.decode(encoding)\n",
    "                detected_encoding = encoding\n",
    "                break\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "                \n",
    "        with open(file_path, 'r', encoding=detected_encoding) as f:\n",
    "            try:\n",
    "                dialect = csv.Sniffer().sniff(f.read(1024))\n",
    "                f.seek(0)\n",
    "            except:\n",
    "                dialect = csv.excel\n",
    "                f.seek(0)\n",
    "                \n",
    "            reader = csv.DictReader(f, dialect=dialect)\n",
    "            data = list(reader)\n",
    "            \n",
    "            if data:\n",
    "                fieldnames = [name.strip().replace(' ', '_').replace('-', '_').lower() \n",
    "                             for name in reader.fieldnames]\n",
    "                data = [{fieldnames[i]: v for i, (k, v) in enumerate(row.items())} \n",
    "                        for row in data]\n",
    "            \n",
    "            return data\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file: {e}\")\n",
    "        raise\n",
    "\n",
    "def validate_row(row: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Validate a row of data before insertion\"\"\"\n",
    "    if 'id' not in row or pd.isna(row['id']):\n",
    "        print(\"Missing required field: id\")\n",
    "        return False\n",
    "        \n",
    "    try:\n",
    "        int(row['id'])\n",
    "    except (ValueError, TypeError):\n",
    "        print(f\"Invalid ID value: {row.get('id')}\")\n",
    "        return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "def prepare_database_values(row: Dict[str, Any]) -> tuple:\n",
    "    \"\"\"Prepare all values for database insertion with proper NULL handling\"\"\"\n",
    "    values = {\n",
    "        'id': int(row['id']),\n",
    "        'timestamp': parse_timestamp(row.get('timestamp'))\n",
    "    }\n",
    "    \n",
    "    for field in FLOAT_FIELDS:\n",
    "        values[field] = clean_float_value(row.get(field), field)\n",
    "    \n",
    "    values.update({\n",
    "        'battery': str(row['battery']) if pd.notna(row.get('battery')) else None,\n",
    "        'cable_pwr': str(row['cable_pwr']) if pd.notna(row.get('cable_pwr')) else None,\n",
    "        'payload': Json(transform_payload(row.get('payload'))) if transform_payload(row.get('payload')) else None,\n",
    "        'device_id': str(row.get('device_id', '')),\n",
    "        'ip': str(row.get('ip', ''))\n",
    "    })\n",
    "    \n",
    "    return tuple(values.values())\n",
    "\n",
    "def insert_batch(cursor, batch: List[Dict[str, Any]]) -> int:\n",
    "    \"\"\"Insert a batch of rows with comprehensive error handling\"\"\"\n",
    "    insert_query = sql.SQL(\"\"\"\n",
    "        INSERT INTO water_quality(\n",
    "            id, timestamp, cond, depth, nlf, odo_sat, odo_cb, odo, \n",
    "            pressure, sal, spcond, tds, turbidity, tss, \n",
    "            wiper_position, temp, vertical_position, battery, \n",
    "            cable_pwr, payload, device_id, ip)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, \n",
    "                %s, %s, %s, %s, %s, %s::jsonb, %s, %s)\n",
    "        ON CONFLICT (id) DO NOTHING\n",
    "    \"\"\")\n",
    "    \n",
    "    success_count = 0\n",
    "    for row in batch:\n",
    "        if not validate_row(row):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            data = prepare_database_values(row)\n",
    "            cursor.execute(insert_query, data)\n",
    "            success_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error inserting row {row.get('id')}: {e}\")\n",
    "            print(f\"Problematic values - Cond: {row.get('cond')}, Wiper_Position: {row.get('wiper_position')}\")\n",
    "            \n",
    "    return success_count\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(CSV_FILE_PATH):\n",
    "        print(f\"Error: CSV file not found at {CSV_FILE_PATH}\")\n",
    "        return\n",
    "\n",
    "    print(\"Starting water quality data import...\")\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        print(\"Reading and processing CSV file...\")\n",
    "        csv_data = process_csv_file(CSV_FILE_PATH)\n",
    "        print(f\"Found {len(csv_data)} rows in CSV file\")\n",
    "        \n",
    "        if not csv_data:\n",
    "            print(\"No data found in CSV file\")\n",
    "            return\n",
    "            \n",
    "        print(\"Connecting to database...\")\n",
    "        db_params = parse_db_url(DATABASE_URL)\n",
    "        conn = psycopg2.connect(**db_params)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        total_success = 0\n",
    "        total_batches = (len(csv_data) // BATCH_SIZE) + 1\n",
    "        \n",
    "        print(f\"\\nStarting batch processing ({total_batches} batches)...\")\n",
    "        for i in range(0, len(csv_data), BATCH_SIZE):\n",
    "            batch = csv_data[i:i+BATCH_SIZE]\n",
    "            batch_num = (i // BATCH_SIZE) + 1\n",
    "            \n",
    "            print(f\"\\nProcessing batch {batch_num}/{total_batches} (rows {i+1}-{min(i+BATCH_SIZE, len(csv_data))})\")\n",
    "            \n",
    "            success = insert_batch(cursor, batch)\n",
    "            conn.commit()\n",
    "            total_success += success\n",
    "            \n",
    "            print(f\"Inserted {success}/{len(batch)} rows successfully\")\n",
    "            print(f\"Overall progress: {min(100, (i + BATCH_SIZE) / len(csv_data) * 100):.1f}%\")\n",
    "        \n",
    "        elapsed = datetime.now() - start_time\n",
    "        print(f\"\\nImport complete in {elapsed.total_seconds():.2f} seconds\")\n",
    "        print(f\"Total rows processed: {len(csv_data)}\")\n",
    "        print(f\"Successfully inserted: {total_success}\")\n",
    "        print(f\"Failed rows: {len(csv_data) - total_success}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nFatal error during import: {e}\")\n",
    "        if 'conn' in locals():\n",
    "            conn.rollback()\n",
    "    finally:\n",
    "        if 'cursor' in locals():\n",
    "            cursor.close()\n",
    "        if 'conn' in locals():\n",
    "            conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
